# Story 5.1: Build LLM Provider Configuration and Management

**Epic**: 5 - AI Chatbot Interface
**Story**: 5.1
**Status**: ðŸŸ¢ Ready for Development
**Estimated Time**: 6-8 hours (1 day)
**Dependencies**: None (first story in Epic 5)

---

## User Story

**As a** user,
**I want** to configure LLM providers globally,
**so that** I can select which models to use for chat.

---

## Acceptance Criteria

### AC1: Database Schema
Alembic migration creates `llm_providers` table:
- `id` (UUID, primary key, default: uuid_generate_v4())
- `provider_name` (ENUM: 'openai', 'google', 'litellm', 'ollama')
- `model_name` (VARCHAR(255), e.g., 'llama3', 'gpt-4')
- `is_default` (BOOLEAN, default: false)
- `api_config` (JSONB, stores non-sensitive config)
- `created_at` (TIMESTAMP, default: CURRENT_TIMESTAMP)

**Constraints**:
- Unique: (provider_name, model_name) - prevent duplicates
- Check: Only one `is_default=true` at a time (enforced in application logic)

### AC2: REST API Endpoints
- `GET /api/llm-providers` - List all providers
- `POST /api/llm-providers` - Create provider
- `PUT /api/llm-providers/{id}` - Update provider
- `DELETE /api/llm-providers/{id}` - Delete provider (block if is_default=true)
- `PUT /api/llm-providers/{id}/set-default` - Set as default

### AC3: API Configuration Storage
Store non-sensitive config in JSONB (API keys in `.env`):
```json
{
  "api_base": "http://localhost:11434",
  "temperature": 0.7,
  "max_tokens": 500
}
```

### AC4: Seed Script
Create default Ollama provider:
```python
{
  "provider_name": "ollama",
  "model_name": "llama3",
  "is_default": true,
  "api_config": {"api_base": "http://localhost:11434"}
}
```

### AC5: Unit Tests
- >70% test coverage
- CRUD operations tested
- Default provider logic tested

---

## Implementation Plan

### Step 1: Create Alembic Migration (15 mins)

**File**: `backend/alembic/versions/{hash}_create_llm_providers_table.py`

```python
"""create_llm_providers_table

Revision ID: {auto-generated}
Revises: 54bd16acd3ce  # Latest Epic 4 migration
Create Date: {auto-generated}

Creates the llm_providers table for Epic 5 (AI Chatbot Interface).
Stores LLM provider configurations for chatbot conversations.
"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects.postgresql import UUID, JSONB

# revision identifiers, used by Alembic.
revision: str = '{auto-generated}'
down_revision: Union[str, Sequence[str], None] = '54bd16acd3ce'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Create llm_providers table."""
    # Create ENUM type for provider names
    op.execute("""
        CREATE TYPE llm_provider_name AS ENUM (
            'openai',
            'google',
            'litellm',
            'ollama'
        )
    """)

    # Create llm_providers table
    op.create_table(
        'llm_providers',
        sa.Column('id', UUID(as_uuid=True), primary_key=True, server_default=sa.text('uuid_generate_v4()')),
        sa.Column('provider_name', sa.Enum('openai', 'google', 'litellm', 'ollama', name='llm_provider_name'), nullable=False),
        sa.Column('model_name', sa.String(255), nullable=False),
        sa.Column('is_default', sa.Boolean, nullable=False, server_default=sa.text('false')),
        sa.Column('api_config', JSONB, nullable=True),
        sa.Column('created_at', sa.DateTime(timezone=True), nullable=False, server_default=sa.text('CURRENT_TIMESTAMP')),
    )

    # Create unique constraint on (provider_name, model_name)
    op.create_unique_constraint(
        'uq_llm_provider_model',
        'llm_providers',
        ['provider_name', 'model_name']
    )

    # Create index on is_default for fast default provider lookup
    op.create_index('ix_llm_providers_is_default', 'llm_providers', ['is_default'])


def downgrade() -> None:
    """Drop llm_providers table."""
    op.drop_index('ix_llm_providers_is_default', table_name='llm_providers')
    op.drop_constraint('uq_llm_provider_model', 'llm_providers', type_='unique')
    op.drop_table('llm_providers')
    op.execute('DROP TYPE llm_provider_name')
```

**Commands**:
```bash
cd backend
alembic revision -m "create_llm_providers_table"
# Edit migration file
alembic upgrade head
```

**Validation**:
```bash
psql -h localhost -U postgres -d bmadflow -c "\d llm_providers"
# Expected: Table with all columns
```

---

### Step 2: Create SQLAlchemy Model (10 mins)

**File**: `backend/app/models/llm_provider.py`

```python
"""LLM Provider model for chatbot configuration."""

from datetime import datetime
from enum import Enum
from typing import Optional
from uuid import UUID, uuid4

from sqlalchemy import Boolean, DateTime, String, func
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy.orm import Mapped, mapped_column

from app.database import Base


class LLMProviderName(str, Enum):
    """Enum for supported LLM providers."""

    OPENAI = "openai"
    GOOGLE = "google"
    LITELLM = "litellm"
    OLLAMA = "ollama"


class LLMProvider(Base):
    """LLM Provider model for chatbot configuration."""

    __tablename__ = "llm_providers"

    id: Mapped[UUID] = mapped_column(primary_key=True, default=uuid4)
    provider_name: Mapped[LLMProviderName] = mapped_column(nullable=False)
    model_name: Mapped[str] = mapped_column(String(255), nullable=False)
    is_default: Mapped[bool] = mapped_column(Boolean, default=False, nullable=False)
    api_config: Mapped[Optional[dict]] = mapped_column(JSONB, nullable=True)
    created_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True), server_default=func.now(), nullable=False
    )

    def __repr__(self) -> str:
        """String representation for debugging."""
        return f"<LLMProvider(id={self.id}, provider={self.provider_name}, model={self.model_name})>"
```

**Update**: `backend/app/models/__init__.py`
```python
from app.models.llm_provider import LLMProvider, LLMProviderName

__all__ = ["LLMProvider", "LLMProviderName", ...]
```

---

### Step 3: Create Pydantic Schemas (15 mins)

**File**: `backend/app/schemas/llm_provider.py`

```python
"""Pydantic schemas for LLM Provider API."""

from datetime import datetime
from typing import Optional
from uuid import UUID

from pydantic import BaseModel, ConfigDict, Field

from app.models.llm_provider import LLMProviderName


class LLMProviderCreate(BaseModel):
    """Schema for creating a new LLM provider."""

    provider_name: LLMProviderName = Field(..., description="LLM provider type")
    model_name: str = Field(..., min_length=1, max_length=255, description="Model name (e.g., llama3, gpt-4)")
    is_default: bool = Field(default=False, description="Set as default provider")
    api_config: Optional[dict] = Field(default=None, description="Non-sensitive API config (JSONB)")


class LLMProviderUpdate(BaseModel):
    """Schema for updating an LLM provider (all fields optional)."""

    model_name: Optional[str] = Field(None, min_length=1, max_length=255)
    api_config: Optional[dict] = None


class LLMProviderResponse(BaseModel):
    """Schema for LLM provider API responses."""

    model_config = ConfigDict(from_attributes=True)

    id: UUID
    provider_name: LLMProviderName
    model_name: str
    is_default: bool
    api_config: Optional[dict]
    created_at: datetime
```

---

### Step 4: Create Repository (30 mins)

**File**: `backend/app/repositories/llm_provider_repository.py`

```python
"""Repository layer for LLM Provider data access."""

from typing import List, Optional
from uuid import UUID

from sqlalchemy import delete, select, update
from sqlalchemy.ext.asyncio import AsyncSession

from app.models.llm_provider import LLMProvider
from app.schemas.llm_provider import LLMProviderCreate, LLMProviderUpdate


class LLMProviderRepository:
    """Repository for LLM Provider CRUD operations."""

    async def create(self, db: AsyncSession, data: LLMProviderCreate) -> LLMProvider:
        """Create a new LLM provider.

        Args:
            db: Database session
            data: LLM provider creation data

        Returns:
            Created LLM provider instance
        """
        # If is_default=True, unset other defaults first
        if data.is_default:
            await self._unset_all_defaults(db)

        provider = LLMProvider(**data.model_dump())
        db.add(provider)
        await db.commit()
        await db.refresh(provider)
        return provider

    async def get_all(self, db: AsyncSession) -> List[LLMProvider]:
        """Get all LLM providers ordered by is_default DESC, created_at DESC.

        Args:
            db: Database session

        Returns:
            List of all LLM providers (default first)
        """
        result = await db.execute(
            select(LLMProvider).order_by(
                LLMProvider.is_default.desc(),
                LLMProvider.created_at.desc()
            )
        )
        return list(result.scalars().all())

    async def get_by_id(self, db: AsyncSession, provider_id: UUID) -> Optional[LLMProvider]:
        """Get LLM provider by ID.

        Args:
            db: Database session
            provider_id: Provider UUID

        Returns:
            LLM provider instance or None if not found
        """
        result = await db.execute(select(LLMProvider).where(LLMProvider.id == provider_id))
        return result.scalar_one_or_none()

    async def get_default(self, db: AsyncSession) -> Optional[LLMProvider]:
        """Get the default LLM provider.

        Args:
            db: Database session

        Returns:
            Default LLM provider or None if no default set
        """
        result = await db.execute(
            select(LLMProvider).where(LLMProvider.is_default == True)
        )
        return result.scalar_one_or_none()

    async def update(
        self, db: AsyncSession, provider_id: UUID, data: LLMProviderUpdate
    ) -> Optional[LLMProvider]:
        """Update LLM provider by ID.

        Args:
            db: Database session
            provider_id: Provider UUID
            data: Update data

        Returns:
            Updated provider instance or None if not found
        """
        provider = await self.get_by_id(db, provider_id)
        if not provider:
            return None

        for key, value in data.model_dump(exclude_unset=True).items():
            setattr(provider, key, value)

        await db.commit()
        await db.refresh(provider)
        return provider

    async def set_default(self, db: AsyncSession, provider_id: UUID) -> Optional[LLMProvider]:
        """Set LLM provider as default (unsets other defaults).

        Args:
            db: Database session
            provider_id: Provider UUID

        Returns:
            Updated provider instance or None if not found
        """
        provider = await self.get_by_id(db, provider_id)
        if not provider:
            return None

        # Unset all other defaults
        await self._unset_all_defaults(db)

        # Set this provider as default
        provider.is_default = True
        await db.commit()
        await db.refresh(provider)
        return provider

    async def delete(self, db: AsyncSession, provider_id: UUID) -> bool:
        """Delete LLM provider by ID.

        Args:
            db: Database session
            provider_id: Provider UUID

        Returns:
            True if deleted, False if not found

        Raises:
            ValueError: If attempting to delete default provider
        """
        provider = await self.get_by_id(db, provider_id)
        if not provider:
            return False

        if provider.is_default:
            raise ValueError("Cannot delete default LLM provider. Set another provider as default first.")

        result = await db.execute(delete(LLMProvider).where(LLMProvider.id == provider_id))
        await db.commit()
        return result.rowcount > 0

    async def _unset_all_defaults(self, db: AsyncSession) -> None:
        """Unset is_default for all providers.

        Args:
            db: Database session
        """
        await db.execute(
            update(LLMProvider).values(is_default=False)
        )
        # Note: No commit here, caller commits
```

---

### Step 5: Create API Router (45 mins)

**File**: `backend/app/routers/llm_providers.py`

```python
"""LLM Provider API router."""

from typing import List
from uuid import UUID

from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.ext.asyncio import AsyncSession

from app.database import get_db
from app.repositories.llm_provider_repository import LLMProviderRepository
from app.schemas.llm_provider import (
    LLMProviderCreate,
    LLMProviderResponse,
    LLMProviderUpdate,
)

router = APIRouter(prefix="/api/llm-providers", tags=["llm-providers"])
repo = LLMProviderRepository()


@router.post("/", response_model=LLMProviderResponse, status_code=status.HTTP_201_CREATED)
async def create_llm_provider(
    data: LLMProviderCreate, db: AsyncSession = Depends(get_db)
) -> LLMProviderResponse:
    """Create a new LLM provider.

    Args:
        data: LLM provider creation data
        db: Database session

    Returns:
        Created LLM provider

    Raises:
        HTTPException: If validation fails (422) or duplicate provider (409)
    """
    try:
        provider = await repo.create(db, data)
        return provider
    except Exception as e:
        if "unique constraint" in str(e).lower():
            raise HTTPException(
                status_code=status.HTTP_409_CONFLICT,
                detail=f"Provider '{data.provider_name}' with model '{data.model_name}' already exists",
            )
        raise


@router.get("/", response_model=List[LLMProviderResponse])
async def list_llm_providers(
    db: AsyncSession = Depends(get_db),
) -> List[LLMProviderResponse]:
    """List all LLM providers (default first, then by created_at DESC).

    Args:
        db: Database session

    Returns:
        List of all LLM providers
    """
    providers = await repo.get_all(db)
    return providers


@router.get("/{provider_id}", response_model=LLMProviderResponse)
async def get_llm_provider(
    provider_id: UUID, db: AsyncSession = Depends(get_db)
) -> LLMProviderResponse:
    """Get LLM provider by ID.

    Args:
        provider_id: Provider UUID
        db: Database session

    Returns:
        LLM provider details

    Raises:
        HTTPException: 404 if provider not found
    """
    provider = await repo.get_by_id(db, provider_id)
    if not provider:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"LLM provider {provider_id} not found",
        )
    return provider


@router.put("/{provider_id}", response_model=LLMProviderResponse)
async def update_llm_provider(
    provider_id: UUID, data: LLMProviderUpdate, db: AsyncSession = Depends(get_db)
) -> LLMProviderResponse:
    """Update LLM provider by ID.

    Args:
        provider_id: Provider UUID
        data: Update data
        db: Database session

    Returns:
        Updated LLM provider

    Raises:
        HTTPException: 404 if provider not found
    """
    provider = await repo.update(db, provider_id, data)
    if not provider:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"LLM provider {provider_id} not found",
        )
    return provider


@router.put("/{provider_id}/set-default", response_model=LLMProviderResponse)
async def set_default_llm_provider(
    provider_id: UUID, db: AsyncSession = Depends(get_db)
) -> LLMProviderResponse:
    """Set LLM provider as default (unsets other defaults).

    Args:
        provider_id: Provider UUID
        db: Database session

    Returns:
        Updated LLM provider (now default)

    Raises:
        HTTPException: 404 if provider not found
    """
    provider = await repo.set_default(db, provider_id)
    if not provider:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"LLM provider {provider_id} not found",
        )
    return provider


@router.delete("/{provider_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_llm_provider(
    provider_id: UUID, db: AsyncSession = Depends(get_db)
) -> None:
    """Delete LLM provider by ID.

    Args:
        provider_id: Provider UUID
        db: Database session

    Raises:
        HTTPException: 404 if provider not found, 400 if default provider
    """
    try:
        deleted = await repo.delete(db, provider_id)
        if not deleted:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"LLM provider {provider_id} not found",
            )
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e),
        )
```

**Update**: `backend/app/main.py`
```python
from app.routers import llm_providers

app.include_router(llm_providers.router)
```

---

### Step 6: Create Seed Script (20 mins)

**File**: `backend/scripts/seed_llm_providers.py`

```python
"""Seed script to create default Ollama LLM provider."""

import asyncio
import os
import sys

# Add backend to Python path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from app.database import AsyncSessionLocal
from app.models.llm_provider import LLMProviderName
from app.repositories.llm_provider_repository import LLMProviderRepository
from app.schemas.llm_provider import LLMProviderCreate


async def seed_default_ollama_provider():
    """Create default Ollama provider if it doesn't exist."""
    async with AsyncSessionLocal() as db:
        repo = LLMProviderRepository()

        # Check if default provider exists
        existing_default = await repo.get_default(db)
        if existing_default:
            print(f"âœ… Default provider already exists: {existing_default.provider_name} - {existing_default.model_name}")
            return

        # Create default Ollama provider
        default_provider = LLMProviderCreate(
            provider_name=LLMProviderName.OLLAMA,
            model_name="llama3",
            is_default=True,
            api_config={
                "api_base": os.getenv("OLLAMA_ENDPOINT_URL", "http://localhost:11434"),
                "temperature": 0.7,
                "max_tokens": 500
            }
        )

        provider = await repo.create(db, default_provider)
        print(f"âœ… Created default LLM provider: {provider.provider_name} - {provider.model_name} (ID: {provider.id})")


if __name__ == "__main__":
    print("ðŸŒ± Seeding default LLM provider...")
    asyncio.run(seed_default_ollama_provider())
    print("âœ… Seed complete!")
```

**Usage**:
```bash
cd backend
python scripts/seed_llm_providers.py
```

---

### Step 7: Write Unit Tests (60 mins)

**File**: `backend/tests/unit/test_llm_provider_repository.py`

```python
"""Unit tests for LLM Provider Repository."""

import pytest
from uuid import uuid4

from app.models.llm_provider import LLMProviderName
from app.repositories.llm_provider_repository import LLMProviderRepository
from app.schemas.llm_provider import LLMProviderCreate, LLMProviderUpdate


@pytest.fixture
def repo():
    """Fixture for LLM Provider repository."""
    return LLMProviderRepository()


@pytest.mark.asyncio
async def test_create_llm_provider(repo, db_session):
    """Test creating an LLM provider."""
    data = LLMProviderCreate(
        provider_name=LLMProviderName.OLLAMA,
        model_name="llama3",
        is_default=True,
        api_config={"api_base": "http://localhost:11434"}
    )

    provider = await repo.create(db_session, data)

    assert provider.id is not None
    assert provider.provider_name == LLMProviderName.OLLAMA
    assert provider.model_name == "llama3"
    assert provider.is_default is True
    assert provider.api_config == {"api_base": "http://localhost:11434"}


@pytest.mark.asyncio
async def test_create_default_provider_unsets_others(repo, db_session):
    """Test that creating a default provider unsets other defaults."""
    # Create first default provider
    data1 = LLMProviderCreate(
        provider_name=LLMProviderName.OLLAMA,
        model_name="llama3",
        is_default=True
    )
    provider1 = await repo.create(db_session, data1)
    assert provider1.is_default is True

    # Create second default provider
    data2 = LLMProviderCreate(
        provider_name=LLMProviderName.OPENAI,
        model_name="gpt-4",
        is_default=True
    )
    provider2 = await repo.create(db_session, data2)
    assert provider2.is_default is True

    # Refresh provider1 and verify it's no longer default
    await db_session.refresh(provider1)
    assert provider1.is_default is False


@pytest.mark.asyncio
async def test_get_all_llm_providers(repo, db_session):
    """Test listing all LLM providers (default first)."""
    # Create providers (non-default first, then default)
    data1 = LLMProviderCreate(
        provider_name=LLMProviderName.OPENAI,
        model_name="gpt-3.5-turbo",
        is_default=False
    )
    await repo.create(db_session, data1)

    data2 = LLMProviderCreate(
        provider_name=LLMProviderName.OLLAMA,
        model_name="llama3",
        is_default=True
    )
    await repo.create(db_session, data2)

    providers = await repo.get_all(db_session)

    assert len(providers) == 2
    # Default provider should be first
    assert providers[0].is_default is True
    assert providers[0].provider_name == LLMProviderName.OLLAMA


@pytest.mark.asyncio
async def test_get_by_id(repo, db_session):
    """Test getting LLM provider by ID."""
    data = LLMProviderCreate(
        provider_name=LLMProviderName.GOOGLE,
        model_name="gemini-pro",
        is_default=False
    )
    created = await repo.create(db_session, data)

    provider = await repo.get_by_id(db_session, created.id)

    assert provider is not None
    assert provider.id == created.id
    assert provider.provider_name == LLMProviderName.GOOGLE


@pytest.mark.asyncio
async def test_get_by_id_not_found(repo, db_session):
    """Test getting non-existent provider returns None."""
    provider = await repo.get_by_id(db_session, uuid4())
    assert provider is None


@pytest.mark.asyncio
async def test_get_default(repo, db_session):
    """Test getting the default LLM provider."""
    data = LLMProviderCreate(
        provider_name=LLMProviderName.OLLAMA,
        model_name="llama3",
        is_default=True
    )
    created = await repo.create(db_session, data)

    default = await repo.get_default(db_session)

    assert default is not None
    assert default.id == created.id
    assert default.is_default is True


@pytest.mark.asyncio
async def test_update_llm_provider(repo, db_session):
    """Test updating LLM provider."""
    data = LLMProviderCreate(
        provider_name=LLMProviderName.OLLAMA,
        model_name="llama2",
        is_default=False
    )
    created = await repo.create(db_session, data)

    update_data = LLMProviderUpdate(model_name="llama3")
    updated = await repo.update(db_session, created.id, update_data)

    assert updated is not None
    assert updated.model_name == "llama3"
    assert updated.provider_name == LLMProviderName.OLLAMA  # Unchanged


@pytest.mark.asyncio
async def test_set_default(repo, db_session):
    """Test setting a provider as default."""
    # Create two providers
    data1 = LLMProviderCreate(
        provider_name=LLMProviderName.OLLAMA,
        model_name="llama3",
        is_default=True
    )
    provider1 = await repo.create(db_session, data1)

    data2 = LLMProviderCreate(
        provider_name=LLMProviderName.OPENAI,
        model_name="gpt-4",
        is_default=False
    )
    provider2 = await repo.create(db_session, data2)

    # Set provider2 as default
    updated = await repo.set_default(db_session, provider2.id)

    assert updated.is_default is True

    # Verify provider1 is no longer default
    await db_session.refresh(provider1)
    assert provider1.is_default is False


@pytest.mark.asyncio
async def test_delete_llm_provider(repo, db_session):
    """Test deleting a non-default LLM provider."""
    data = LLMProviderCreate(
        provider_name=LLMProviderName.GOOGLE,
        model_name="gemini-pro",
        is_default=False
    )
    created = await repo.create(db_session, data)

    deleted = await repo.delete(db_session, created.id)

    assert deleted is True

    # Verify provider no longer exists
    provider = await repo.get_by_id(db_session, created.id)
    assert provider is None


@pytest.mark.asyncio
async def test_delete_default_provider_raises_error(repo, db_session):
    """Test deleting default provider raises ValueError."""
    data = LLMProviderCreate(
        provider_name=LLMProviderName.OLLAMA,
        model_name="llama3",
        is_default=True
    )
    created = await repo.create(db_session, data)

    with pytest.raises(ValueError, match="Cannot delete default LLM provider"):
        await repo.delete(db_session, created.id)
```

**File**: `backend/tests/integration/test_llm_provider_api.py`

```python
"""Integration tests for LLM Provider API endpoints."""

import pytest
from httpx import AsyncClient

from app.models.llm_provider import LLMProviderName


@pytest.mark.asyncio
async def test_create_llm_provider(async_client: AsyncClient):
    """Test POST /api/llm-providers creates a provider."""
    response = await async_client.post(
        "/api/llm-providers",
        json={
            "provider_name": "ollama",
            "model_name": "llama3",
            "is_default": True,
            "api_config": {"api_base": "http://localhost:11434"}
        }
    )

    assert response.status_code == 201
    data = response.json()
    assert data["provider_name"] == "ollama"
    assert data["model_name"] == "llama3"
    assert data["is_default"] is True
    assert data["id"] is not None


@pytest.mark.asyncio
async def test_list_llm_providers(async_client: AsyncClient):
    """Test GET /api/llm-providers lists all providers."""
    # Create provider first
    await async_client.post(
        "/api/llm-providers",
        json={"provider_name": "ollama", "model_name": "llama3", "is_default": True}
    )

    response = await async_client.get("/api/llm-providers")

    assert response.status_code == 200
    data = response.json()
    assert len(data) >= 1
    assert data[0]["is_default"] is True  # Default first


@pytest.mark.asyncio
async def test_get_llm_provider_by_id(async_client: AsyncClient):
    """Test GET /api/llm-providers/{id} returns provider."""
    # Create provider
    create_response = await async_client.post(
        "/api/llm-providers",
        json={"provider_name": "google", "model_name": "gemini-pro"}
    )
    provider_id = create_response.json()["id"]

    response = await async_client.get(f"/api/llm-providers/{provider_id}")

    assert response.status_code == 200
    data = response.json()
    assert data["id"] == provider_id
    assert data["provider_name"] == "google"


@pytest.mark.asyncio
async def test_update_llm_provider(async_client: AsyncClient):
    """Test PUT /api/llm-providers/{id} updates provider."""
    # Create provider
    create_response = await async_client.post(
        "/api/llm-providers",
        json={"provider_name": "ollama", "model_name": "llama2"}
    )
    provider_id = create_response.json()["id"]

    # Update
    response = await async_client.put(
        f"/api/llm-providers/{provider_id}",
        json={"model_name": "llama3"}
    )

    assert response.status_code == 200
    data = response.json()
    assert data["model_name"] == "llama3"


@pytest.mark.asyncio
async def test_set_default_llm_provider(async_client: AsyncClient):
    """Test PUT /api/llm-providers/{id}/set-default sets provider as default."""
    # Create two providers
    response1 = await async_client.post(
        "/api/llm-providers",
        json={"provider_name": "ollama", "model_name": "llama3", "is_default": True}
    )
    provider1_id = response1.json()["id"]

    response2 = await async_client.post(
        "/api/llm-providers",
        json={"provider_name": "openai", "model_name": "gpt-4"}
    )
    provider2_id = response2.json()["id"]

    # Set provider2 as default
    response = await async_client.put(f"/api/llm-providers/{provider2_id}/set-default")

    assert response.status_code == 200
    data = response.json()
    assert data["is_default"] is True

    # Verify provider1 no longer default
    response1 = await async_client.get(f"/api/llm-providers/{provider1_id}")
    assert response1.json()["is_default"] is False


@pytest.mark.asyncio
async def test_delete_llm_provider(async_client: AsyncClient):
    """Test DELETE /api/llm-providers/{id} deletes non-default provider."""
    # Create non-default provider
    create_response = await async_client.post(
        "/api/llm-providers",
        json={"provider_name": "google", "model_name": "gemini-pro", "is_default": False}
    )
    provider_id = create_response.json()["id"]

    # Delete
    response = await async_client.delete(f"/api/llm-providers/{provider_id}")

    assert response.status_code == 204

    # Verify deleted
    get_response = await async_client.get(f"/api/llm-providers/{provider_id}")
    assert get_response.status_code == 404


@pytest.mark.asyncio
async def test_delete_default_provider_returns_400(async_client: AsyncClient):
    """Test deleting default provider returns 400 error."""
    # Create default provider
    create_response = await async_client.post(
        "/api/llm-providers",
        json={"provider_name": "ollama", "model_name": "llama3", "is_default": True}
    )
    provider_id = create_response.json()["id"]

    # Attempt delete
    response = await async_client.delete(f"/api/llm-providers/{provider_id}")

    assert response.status_code == 400
    assert "Cannot delete default" in response.json()["detail"]
```

**Run Tests**:
```bash
cd backend
python -m pytest tests/unit/test_llm_provider_repository.py -v
python -m pytest tests/integration/test_llm_provider_api.py -v
python -m pytest tests/unit/test_llm_provider_repository.py tests/integration/test_llm_provider_api.py --cov=app.repositories.llm_provider_repository --cov=app.routers.llm_providers --cov-report=term-missing
```

---

### Step 8: Production Validation (15 mins)

```bash
# Start backend
cd backend
uvicorn app.main:app --reload

# In another terminal, test endpoints:

# 1. Create default Ollama provider (seed)
python scripts/seed_llm_providers.py

# 2. List providers
curl http://localhost:8000/api/llm-providers | jq

# 3. Create OpenAI provider
curl -X POST http://localhost:8000/api/llm-providers \
  -H "Content-Type: application/json" \
  -d '{
    "provider_name": "openai",
    "model_name": "gpt-4",
    "is_default": false,
    "api_config": {"temperature": 0.7}
  }' | jq

# 4. Set OpenAI as default
PROVIDER_ID="<id from step 3>"
curl -X PUT http://localhost:8000/api/llm-providers/$PROVIDER_ID/set-default | jq

# 5. Verify default changed
curl http://localhost:8000/api/llm-providers | jq

# 6. Try to delete default (should fail)
curl -X DELETE http://localhost:8000/api/llm-providers/$PROVIDER_ID

# Expected: 400 error "Cannot delete default"

# 7. Delete non-default provider
# (Get Ollama provider ID from list)
curl -X DELETE http://localhost:8000/api/llm-providers/<ollama-id>

# Expected: 204 No Content
```

**Validation Checklist**:
- [ ] Seed script creates default Ollama provider
- [ ] GET /api/llm-providers returns providers (default first)
- [ ] POST /api/llm-providers creates provider
- [ ] PUT /api/llm-providers/{id}/set-default works
- [ ] DELETE default provider returns 400 error
- [ ] DELETE non-default provider returns 204
- [ ] OpenAPI docs updated: http://localhost:8000/api/docs

---

### Step 9: Code Formatting & QA Gate (10 mins)

```bash
cd backend

# Format code
black app/models/llm_provider.py app/schemas/llm_provider.py app/repositories/llm_provider_repository.py app/routers/llm_providers.py scripts/seed_llm_providers.py
ruff check app/models/llm_provider.py app/schemas/llm_provider.py app/repositories/llm_provider_repository.py app/routers/llm_providers.py

# Run full test suite
python -m pytest tests/unit/test_llm_provider_repository.py tests/integration/test_llm_provider_api.py -v --cov=app --cov-report=term-missing

# Expected: >70% coverage, all tests passing
```

**QA Gate Checklist**:
- [ ] All 5 acceptance criteria met
- [ ] >70% test coverage
- [ ] Black/Ruff formatting (100% compliance)
- [ ] Type hints on all functions
- [ ] Google-style docstrings
- [ ] OpenAPI documentation updated
- [ ] Production validation passed
- [ ] Ready for Story 5.2

---

## Estimated Timeline

| Step | Time | Cumulative |
|------|------|------------|
| 1. Alembic Migration | 15 min | 0:15 |
| 2. SQLAlchemy Model | 10 min | 0:25 |
| 3. Pydantic Schemas | 15 min | 0:40 |
| 4. Repository | 30 min | 1:10 |
| 5. API Router | 45 min | 1:55 |
| 6. Seed Script | 20 min | 2:15 |
| 7. Unit Tests | 60 min | 3:15 |
| 8. Production Validation | 15 min | 3:30 |
| 9. Formatting & QA | 10 min | 3:40 |

**Total**: ~4 hours (buffer: 6-8 hours for Story 5.1)

---

## Success Criteria

**Story 5.1 Complete When**:
- âœ… All 5 acceptance criteria met
- âœ… Database migration applied successfully
- âœ… All CRUD endpoints working
- âœ… Default Ollama provider seeded
- âœ… >70% test coverage achieved
- âœ… Production validation passed
- âœ… Code formatted (Black/Ruff)
- âœ… OpenAPI documentation updated
- âœ… QA gate passed

---

## Next Story

**Story 5.2**: Implement Pydantic Agent Framework for RAG (Most complex story in Epic 5)

**Preparation for 5.2**:
- [ ] Install LiteLLM: `pip install litellm`
- [ ] Install Pydantic AI: `pip install pydantic-ai`
- [ ] Define system prompt template
- [ ] Test vector search API (Epic 4)

---

**Story Status**: ðŸŸ¢ Ready for Development
**Approved By**: Product Manager (John)
**Date**: 2025-10-13
