# Story 4.1: Integrate Docling Document Processing

## Status

Approved

## Story

**As a** developer,
**I want** to process synced documents using Docling's HybridChunker,
**so that** I can generate optimized chunks for RAG.

## Acceptance Criteria

1. Docling library integrated into backend (`requirements.txt`: `docling>=1.0.0`)
2. `DoclingService` created in `app/services/docling_service.py` using HybridChunker
3. Method `process_markdown(content: str) -> List[Chunk]`: serialize, chunk, extract metadata
4. Method `process_csv(content: str) -> List[Chunk]`: serialize, chunk by rows
5. Method `process_yaml_json(content: str, file_type: str) -> List[Chunk]`: chunk by structure
6. HybridChunker configured with Ollama tokenization using model `all-MiniLM-L6-v2`
7. Unit tests: 5+ tests in `tests/unit/services/test_docling_service.py`
8. Integration test: `tests/integration/test_document_processing.py` - process 10+ BMAD files

## Tasks / Subtasks

- [ ] **Task 1: Install Docling library and validate setup** (AC: 1)
  - [ ] Add `docling>=1.0.0` to `backend/requirements.txt`
  - [ ] Run `pip install docling` in backend environment
  - [ ] Verify installation: `python -c "import docling; print('OK')"`
  - [ ] Review Docling documentation for HybridChunker API

- [ ] **Task 2: Create DoclingService with HybridChunker** (AC: 2, 6)
  - [ ] Create `backend/app/services/docling_service.py`
  - [ ] Implement `DoclingService` class with HybridChunker initialization
  - [ ] Configure HybridChunker with Ollama tokenizer: `tokenizer="ollama", tokenizer_model="all-MiniLM-L6-v2"`
  - [ ] Add structured logging for chunk counts

- [ ] **Task 3: Implement process_markdown method** (AC: 3)
  - [ ] Create method signature: `async def process_markdown(self, content: str) -> List[Chunk]`
  - [ ] Load markdown content into Docling processor
  - [ ] Chunk using HybridChunker
  - [ ] Extract metadata: chunk index, total chunks, position
  - [ ] Return list of Chunk objects

- [ ] **Task 4: Implement process_csv method** (AC: 4)
  - [ ] Create method signature: `async def process_csv(self, content: str) -> List[Chunk]`
  - [ ] Parse CSV content with Docling
  - [ ] Chunk by rows or logical groups
  - [ ] Extract metadata including CSV schema info
  - [ ] Return list of Chunk objects

- [ ] **Task 5: Implement process_yaml_json method** (AC: 5)
  - [ ] Create method signature: `async def process_yaml_json(self, content: str, file_type: str) -> List[Chunk]`
  - [ ] Parse YAML/JSON structure with Docling
  - [ ] Chunk by structural elements (objects, arrays)
  - [ ] Extract metadata including file_type
  - [ ] Return list of Chunk objects

- [ ] **Task 6: Create Chunk schema model** (AC: 2, 3, 4, 5)
  - [ ] Create `backend/app/schemas/chunk.py`
  - [ ] Define `ChunkResponse` Pydantic model with fields: text, index, metadata
  - [ ] Add type hints: `text: str`, `index: int`, `metadata: dict`
  - [ ] Document metadata structure in docstring

- [ ] **Task 7: Add error handling for unsupported file types** (AC: 3, 4, 5)
  - [ ] Raise `ValueError` with clear message for unsupported file types
  - [ ] Log errors with `logger.error()` including file type
  - [ ] Add type validation in each process method

- [ ] **Task 8: Write unit tests** (AC: 7)
  - [ ] Create `backend/tests/unit/services/test_docling_service.py`
  - [ ] Test: `test_process_markdown_simple` - Basic markdown with headers
  - [ ] Test: `test_process_markdown_code_blocks` - Code fence handling
  - [ ] Test: `test_process_csv_with_headers` - CSV with header row
  - [ ] Test: `test_process_yaml_structure` - Nested YAML chunking
  - [ ] Test: `test_process_json_structure` - JSON array/object chunking
  - [ ] Test: `test_unsupported_file_type` - Raises ValueError
  - [ ] Verify 80%+ coverage for DoclingService using `pytest-cov`

- [ ] **Task 9: Write integration tests** (AC: 8)
  - [ ] Create `backend/tests/integration/test_document_processing.py`
  - [ ] Test: `test_process_bmad_prd_markdown` - Process actual PRD.md from BMAD docs
  - [ ] Test: `test_process_multiple_file_types` - Mix of MD, CSV, YAML files
  - [ ] Test: `test_chunk_metadata_complete` - Verify all metadata fields present
  - [ ] Verify 70%+ overall integration test coverage

- [ ] **Task 10: Apply code quality checks** (AC: All)
  - [ ] Run `black app/ tests/` to format code (line length 100)
  - [ ] Run `ruff check app/ tests/ --fix` to lint code
  - [ ] Add type hints to all functions: `List[Chunk]`, `str`, `dict`
  - [ ] Add Google-style docstrings to all public methods
  - [ ] Verify no errors from Ruff linting

## Dev Notes

### Architecture Context

**Service Layer Location**: `backend/app/services/docling_service.py`
[Source: docs/architecture/source-tree.md#backend-structure]

**Schema Location**: `backend/app/schemas/chunk.py`
[Source: docs/architecture/source-tree.md#backend-structure]

### Tech Stack

**Document Processing Library**: Docling (latest version, >=1.0.0)
**Chunking Strategy**: HybridChunker with Ollama tokenization
**Tokenizer Configuration**:
- `tokenizer="ollama"`
- `tokenizer_model="all-MiniLM-L6-v2"`
**Supported File Types**: Markdown (.md), CSV (.csv), YAML (.yaml, .yml), JSON (.json)
[Source: docs/architecture/tech-stack.md#technology-stack-table]

### Data Model - Chunk Schema

```python
# backend/app/schemas/chunk.py
from pydantic import BaseModel

class Chunk(BaseModel):
    text: str                    # Chunk content
    index: int                   # Order within document (0-based)
    metadata: dict              # Contains: file_path, file_type, position, total_chunks
```

[Source: docs/epics/epic-4-rag-knowledge-base-vector-search.md#story-41-integrate-docling-library-and-document-processing-pipeline]

### Error Handling

- **Unsupported File Types**: Raise `ValueError` with message specifying unsupported type
- **Logging**: Use `logger.error()` for errors, `logger.info()` for chunk counts
- **Error Format**: Clear, actionable messages for developers

[Source: docs/architecture/backend-architecture.md#error-handling-patterns]

### Docling Integration Pattern

```python
from docling import DocumentProcessor
from docling.chunking import HybridChunker

# Initialize in DoclingService with Ollama tokenization
processor = DocumentProcessor()
chunker = HybridChunker(
    tokenizer="ollama",
    tokenizer_model="all-MiniLM-L6-v2"
)

# Process document
def process_document(content: str, file_type: str) -> List[Chunk]:
    doc = processor.load_from_text(content, file_type=file_type)
    chunks = chunker.chunk(doc)
    return chunks
```

**Tokenizer Configuration:**
- Uses Ollama for tokenization to ensure chunk boundaries align with embedding model
- Model `all-MiniLM-L6-v2` matches the tokenization strategy for optimal RAG performance
- HybridChunker automatically determines optimal chunk sizes based on tokenizer

[Source: docs/architecture/backend-architecture.md#docling-integration]

### Testing

#### Test File Locations

- **Unit Tests**: `backend/tests/unit/services/test_docling_service.py`
- **Integration Tests**: `backend/tests/integration/test_document_processing.py`
- **Test Fixtures**: Use sample markdown/CSV/YAML from `backend/tests/fixtures/`

[Source: docs/architecture/source-tree.md#backend-structure]

#### Testing Standards

**Framework**: pytest with async support
**Coverage Tool**: pytest-cov
**Coverage Target**: 80%+ for DoclingService, 70%+ overall

**Unit Test Pattern**:
```python
import pytest
from app.services.docling_service import DoclingService

@pytest.mark.asyncio
async def test_process_markdown_simple():
    # Arrange
    service = DoclingService()
    markdown_content = "# Header\n\nParagraph text"

    # Act
    chunks = await service.process_markdown(markdown_content)

    # Assert
    assert len(chunks) > 0
    assert chunks[0].text is not None
    assert chunks[0].metadata['file_type'] == 'md'
```

[Source: docs/architecture/testing-strategy.md#backend-testing]

#### Required Test Cases

**Unit Tests** (6 minimum):
1. `test_process_markdown_simple` - Basic markdown with headers
2. `test_process_markdown_code_blocks` - Code fence handling
3. `test_process_csv_with_headers` - CSV with header row
4. `test_process_yaml_structure` - Nested YAML chunking
5. `test_process_json_structure` - JSON array/object chunking
6. `test_unsupported_file_type` - Raises ValueError

**Integration Tests** (3 minimum):
1. `test_process_bmad_prd_markdown` - Process actual PRD.md
2. `test_process_multiple_file_types` - Mix of MD, CSV, YAML
3. `test_chunk_metadata_complete` - Verify all metadata fields

[Source: docs/epics/epic-4-rag-knowledge-base-vector-search.md#story-41-testing-requirements]

#### Running Tests

```bash
# Unit tests with coverage
cd backend
pytest tests/unit/services/test_docling_service.py -v --cov=app.services.docling_service

# Integration tests
pytest tests/integration/test_document_processing.py -v

# All tests with HTML coverage report
pytest tests/ -v --cov=app --cov-report=html
```

[Source: docs/architecture/testing-strategy.md#measuring-coverage]

### Coding Standards

**Python Formatting**: Black with line length 100
**Linting**: Ruff with target Python 3.11+
**Type Hints**: Required on all functions (`List`, `Dict`, `str`, `int`)
**Docstrings**: Google style for all public methods

**Example Method Signature**:
```python
async def process_markdown(self, content: str) -> List[Chunk]:
    """Process markdown content into chunks.

    Args:
        content: Raw markdown content as string

    Returns:
        List of Chunk objects with text, index, and metadata

    Raises:
        ValueError: If content is empty or invalid
    """
    pass
```

[Source: docs/architecture/coding-standards.md#python-backend]

### Code Quality Checklist

Before marking story complete, verify:
- [ ] Black formatting applied: `black app/ tests/`
- [ ] Ruff linting passed: `ruff check app/ tests/ --fix` (no errors)
- [ ] Type hints on all functions
- [ ] Google-style docstrings on all public methods
- [ ] Structured logging for chunk counts (`logger.info(f"Generated {count} chunks")`)
- [ ] Error messages clear and actionable

[Source: docs/epics/epic-4-rag-knowledge-base-vector-search.md#code-quality-checklist]

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-09 | 1.0 | Initial story creation from Epic 4 | Bob (Scrum Master) |
| 2025-10-09 | 1.1 | Updated HybridChunker config to use Ollama tokenization with all-MiniLM-L6-v2 | Bob (Scrum Master) |
| 2025-10-09 | 1.2 | PO Validation: APPROVED - Score 10/10, all validations passed | Sarah (Product Owner) |

## Dev Agent Record

_This section will be populated by the development agent during implementation._

### Agent Model Used

_To be filled by dev agent_

### Debug Log References

_To be filled by dev agent_

### Completion Notes List

_To be filled by dev agent_

### File List

_To be filled by dev agent_

## QA Results

_This section will be populated by the QA agent after story implementation._
